{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHKUu2ckTc9i"
      },
      "outputs": [],
      "source": [
        "\"\"\"!pip install -q transformers\n",
        "!pip install keras-tuner -q\n",
        "!pip install visualkeras\n",
        "!pip install pydot\n",
        "!pip install graphviz\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from google.colab import files\n",
        "from transformers import TFBertModel, BertTokenizer, BertConfig\n",
        "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, classification_report\n",
        "from keras.optimizers import Adam\n",
        "from keras import utils\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Concatenate, AveragePooling2D, Dropout\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnKXtZ4lTlxv"
      },
      "outputs": [],
      "source": [
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzkiOPFSTrO6"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert_bfd\", do_lower_case=False )\n",
        "model = TFBertModel.from_pretrained(\"Rostlab/prot_bert_bfd\", from_pt=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QU59GO9UWmz"
      },
      "outputs": [],
      "source": [
        "uploaded_fasta_train = list(uploaded.keys())[2]\n",
        "fasta_content_train = uploaded[uploaded_fasta_train].decode('utf-8')  # Decode the bytes to a string\n",
        "print(uploaded_fasta_train)\n",
        "print(len(fasta_content_train))\n",
        "#print(fasta_content_train)\n",
        "\n",
        "uploaded_fasta_test = list(uploaded.keys())[1]\n",
        "fasta_content_test = uploaded[uploaded_fasta_test].decode('utf-8')  # Decode the bytes to a string\n",
        "print(uploaded_fasta_test)\n",
        "print(len(fasta_content_test))\n",
        "#print(fasta_content_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnOrU3AL2bUJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9rf0qLrdKKG"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpWjTsr_UwqG"
      },
      "outputs": [],
      "source": [
        "#Reading Train Dataset with FASTA format\n",
        "sequences_train = {}\n",
        "current_header_train = None\n",
        "current_sequence_train = []\n",
        "\n",
        "#Parse train\n",
        "for line in fasta_content_train.split('\\n'):\n",
        "    line = line.strip()\n",
        "    if line.startswith(\">\"):  # Header line\n",
        "        if current_header_train is not None:\n",
        "            sequences_train[current_header_train] = ''.join(current_sequence_train)\n",
        "        current_header_train = line[1:]  # Remove the '>' character\n",
        "        current_sequence_train = []\n",
        "    else:\n",
        "        current_sequence_train.append(line)\n",
        "\n",
        "#Don't forget to add the last sequence\n",
        "if current_header_train is not None:\n",
        "    sequences_train[current_header_train] = ''.join(current_sequence_train)\n",
        "\n",
        "labels_train = []\n",
        "\n",
        "for header in sequences_train.keys():\n",
        "    if \"Positive\" in header:\n",
        "        labels_train.append(\"Positive\")\n",
        "    elif \"Negative\" in header:\n",
        "        labels_train.append(\"Negative\")\n",
        "\n",
        "y_train = [label == 'Positive' for label in labels_train]\n",
        "print(\"Toplam pozitif train örnek sayısı:\", sum(y_train))\n",
        "print(\"Toplam negatif train örnek sayısı:\", len(y_train) - sum(y_train))\n",
        "print(\"length of y_train:\", len(y_train))\n",
        "#print(y_train)\n",
        "\n",
        "#----------------------------------------\n",
        "print(\"--------------------------\")\n",
        "\n",
        "#Reading Test Dataset with FASTA format\n",
        "sequences_test = {}\n",
        "current_header_test = None\n",
        "current_sequence_test = []\n",
        "\n",
        "# Parse the FASTA content test\n",
        "for line in fasta_content_test.split('\\n'):\n",
        "    line = line.strip()\n",
        "    if line.startswith(\">\"):  # Header line\n",
        "        if current_header_test is not None:\n",
        "            sequences_test[current_header_test] = ''.join(current_sequence_test)\n",
        "        current_header_test = line[1:]  # Remove the '>' character\n",
        "        current_sequence_test = []\n",
        "    else:\n",
        "        current_sequence_test.append(line)\n",
        "\n",
        "# Don't forget to add the last sequence\n",
        "if current_header_test is not None:\n",
        "    sequences_test[current_header_test] = ''.join(current_sequence_test)\n",
        "\n",
        "# pozitif veya negatif diye sırasıyla alan array\n",
        "labels_test = []\n",
        "\n",
        "for header in sequences_test.keys():\n",
        "    if \"Positive\" in header:\n",
        "        labels_test.append(\"Positive\")\n",
        "    elif \"Negative\" in header:\n",
        "        labels_test.append(\"Negative\")\n",
        "\n",
        "print(\"Toplam pozitif train örnek sayısı:\", sum(y_train))\n",
        "print(\"Toplam negatif train örnek sayısı:\", len(y_train) - sum(y_train))\n",
        "y_test = [label == 'Positive' for label in labels_test]\n",
        "print(\"length of the y_test: \", len(y_test))\n",
        "#print(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnN9Lgx_Weyk"
      },
      "outputs": [],
      "source": [
        "sequences_Example_train = []\n",
        "length_train = []\n",
        "\n",
        "for sequence in sequences_train.values():\n",
        "    formatted_sequence = ' '.join(sequence)\n",
        "\n",
        "    sequences_Example_train.append(formatted_sequence)\n",
        "    length_train.append(len(sequence))\n",
        "\n",
        "#print(length_train)\n",
        "print(\"Min Train Sequence Length:\", min(length_train))\n",
        "print(\"Max Train Sequence Length:\", max(length_train))\n",
        "print(\"Train sequences:\", sequences_Example_train)\n",
        "\n",
        "sequences_Example_test = []\n",
        "length_test = []\n",
        "\n",
        "for sequence in sequences_test.values():\n",
        "    formatted_sequence = ' '.join(sequence)\n",
        "    sequences_Example_test.append(formatted_sequence)\n",
        "    length_test.append(len(sequence))\n",
        "\n",
        "\n",
        "#print(length_test)\n",
        "print(\"Min Test Sequence Length:\", min(length_test))\n",
        "print(\"Max Test Sequence Length:\",max(length_test))\n",
        "print(\"Test sequences:\", sequences_Example_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Buw7sM7eWhXw"
      },
      "outputs": [],
      "source": [
        "#Combining X and y by zipping them together\n",
        "combined_data = list(zip(sequences_Example_train,y_train))\n",
        "random.seed(42)\n",
        "random.shuffle(combined_data)\n",
        "\n",
        "#Seperate mixed X and y pairs\n",
        "sequences_Example_train, y_train = zip(*combined_data)\n",
        "\n",
        "print(\"Mixed X_train Sequences:\", sequences_Example_train)\n",
        "print(\"Mixed y_train Values:\", y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU8tPOz_bov1"
      },
      "outputs": [],
      "source": [
        "#Extracting feature embeddings for X_train\n",
        "sequences_Example_train = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences_Example_train]\n",
        "ids_train = tokenizer.batch_encode_plus(sequences_Example_train, add_special_tokens=True, padding=True, return_tensors=\"tf\")\n",
        "input_ids = ids_train['input_ids']\n",
        "attention_mask = ids_train['attention_mask']\n",
        "embedding = model(input_ids)[0]\n",
        "embedding = np.asarray(embedding)\n",
        "attention_mask = np.asarray(attention_mask)\n",
        "X_train = []\n",
        "for seq_num in range(len(embedding)):\n",
        "    seq_len = (attention_mask[seq_num] == 1).sum()\n",
        "    seq_emd = embedding[seq_num][1:seq_len-1]\n",
        "    X_train.append(seq_emd)\n",
        "\n",
        "\n",
        "#print(X_train)\n",
        "print(\"Length of X_train:\", len(X_train))\n",
        "\n",
        "\n",
        "sequences_Example_test = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences_Example_test]\n",
        "ids_test = tokenizer.batch_encode_plus(sequences_Example_test, add_special_tokens=True, padding=True, return_tensors=\"tf\")\n",
        "\n",
        "input_ids_test = ids_test['input_ids']\n",
        "attention_mask_test = ids_test['attention_mask']\n",
        "embedding_test = model(input_ids_test)[0]\n",
        "embedding_test = np.asarray(embedding_test)\n",
        "attention_mask_test = np.asarray(attention_mask_test)\n",
        "X_test = []\n",
        "for seq_num in range(len(embedding_test)):\n",
        "    seq_len = (attention_mask_test[seq_num] == 1).sum()\n",
        "    seq_emd = embedding_test[seq_num][1:seq_len-1]\n",
        "    X_test.append(seq_emd)\n",
        "\n",
        "#print(X_test)\n",
        "print(\"Length of X_test:\", len(X_test))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wX0SFZPdVEL"
      },
      "outputs": [],
      "source": [
        "#Adding new 1024 features by taking means and stdev for each sequence (1024 + 1024 = 2048 features for every sequence)\n",
        "\n",
        "result_matrix_train = []\n",
        "for i in range(len(X_train)):\n",
        "    mean_matrix_train = np.mean(X_train[i], axis=0, keepdims=True)\n",
        "    std_matrix_train = np.std(X_train[i], axis=0, keepdims=True)\n",
        "    result_matrix_train.append(np.hstack((mean_matrix_train, std_matrix_train)) )\n",
        "\n",
        "X_train_final = np.vstack(result_matrix_train)\n",
        "\n",
        "print(\"Shape of new train features:\", X_train_final.shape)\n",
        "\n",
        "\n",
        "result_matrix_test = []\n",
        "for i in range(len(X_test)):\n",
        "    mean_matrix_test = np.mean(X_test[i], axis=0, keepdims=True)\n",
        "    std_matrix_test = np.std(X_test[i], axis=0, keepdims=True)\n",
        "    result_matrix_test.append(np.hstack((mean_matrix_test, std_matrix_test)) )\n",
        "\n",
        "X_test_final = np.vstack(result_matrix_test)\n",
        "\n",
        "\n",
        "print(\"Shape of new test features:\", X_test_final.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BVQwGVnX_f2"
      },
      "outputs": [],
      "source": [
        "mean_matrix_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41ZN5eNtc6dr"
      },
      "source": [
        "## iTTCA-RF (Chemical) Features Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4JEKPaebpAD"
      },
      "outputs": [],
      "source": [
        "#ITTCA-RF- Feature extraction\n",
        "#!/usr/bin/env python\n",
        "#_*_coding:utf-8_*_\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "\n",
        "sequences_Example_train = [sequence.replace(\" \", \"\") for sequence in sequences_Example_train]\n",
        "fastas_train = sequences_Example_train\n",
        "\n",
        "sequences_Example_test = [sequence.replace(\" \", \"\") for sequence in sequences_Example_test]\n",
        "fastas_test = sequences_Example_test\n",
        "\n",
        "def Count_1(seq1, seq2):\n",
        "    sum = 0\n",
        "    for aa in seq1:\n",
        "        sum = sum + seq2.count(aa)\n",
        "    return sum\n",
        "def Count_2(aaSet, sequence):\n",
        "    number = 0\n",
        "    for aa in sequence:\n",
        "        if aa in aaSet:\n",
        "            number = number + 1\n",
        "    cutoffNums = [1, math.floor(0.25 * number), math.floor(0.50 * number), math.floor(0.75 * number), number]\n",
        "    cutoffNums = [i if i >=1 else 0 for i in cutoffNums]\n",
        "    code = []\n",
        "    for cutoff in cutoffNums:\n",
        "        myCount = 0\n",
        "        if cutoff == 0:\n",
        "            code.append(0)\n",
        "        else:\n",
        "            for i in range(len(sequence)):\n",
        "                if sequence[i] in aaSet:\n",
        "                    myCount += 1\n",
        "                    if myCount == cutoff:\n",
        "                        code.append((i + 1) / len(sequence) * 100)\n",
        "                        break\n",
        "            if myCount == 0:\n",
        "                code.append(0)\n",
        "    return code\n",
        "\n",
        "AA = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "group = {\n",
        "        'alphaticr': 'GAVLMI',\n",
        "        'aromatic': 'FYW',\n",
        "        'postivecharger': 'KRH',\n",
        "        'negativecharger': 'DE',\n",
        "        'uncharger': 'STCPNQ'}\n",
        "\n",
        "group1 = {\n",
        "        'hydrophobicity': 'RKEDQN',  # 疏水性特征\n",
        "        'normwaalsvolume': 'GASCTPD',  # 范德华力\n",
        "        'polarity': 'LIFWCMVY',  # 极性\n",
        "        'polarizability': 'GASDT',  # 极化性质\n",
        "        'charge': 'KR',  # 电荷性质\n",
        "        'surfacetension': 'GQDNAHR',  # 表面张力\n",
        "        'secondarystruct': 'EALMQKRH',  # 二级结构\n",
        "        'solventaccess': 'ALFCGIVW'}  # 溶剂可及性\n",
        "\n",
        "group2 = {\n",
        "        'hydrophobicity': 'GASTPHY',\n",
        "        'normwaalsvolume': 'NVEQIL',\n",
        "        'polarity': 'PATGS',\n",
        "        'polarizability': 'CPNVEQIL',\n",
        "        'charge': 'ANCQGHILMFPSTWYV',\n",
        "        'surfacetension': 'KTSEC',\n",
        "        'secondarystruct': 'VIYCWFT',\n",
        "        'solventaccess': 'RKQEND'}\n",
        "group3 = {\n",
        "        'hydrophobicity': 'CVLIMFW',\n",
        "        'normwaalsvolume': 'MHKFRYW',\n",
        "        'polarity': 'HQRKNED',\n",
        "        'polarizability': 'KMHFRYW',\n",
        "        'charge': 'DE',\n",
        "        'surfacetension': 'ILMFPWYV',\n",
        "        'secondarystruct': 'GNPSD',\n",
        "        'solventaccess': 'MPSTHY'}\n",
        "property = ('hydrophobicity', 'normwaalsvolume',\n",
        "                'polarity', 'polarizability', 'charge', 'surfacetension', 'secondarystruct', 'solventaccess')\n",
        "\n",
        "def get_feature_names():\n",
        "    AA = 'ACDEFGHIKLMNPQRSTVWY'  # AA değişkenini burada tanımlıyoruz.\n",
        "    feature_names = []\n",
        "\n",
        "    # AAC features\n",
        "    feature_names.extend([f\"AAC_{aa}\" for aa in AA])\n",
        "\n",
        "    # CTDC, CTDT, CTDD features\n",
        "    for p in property:\n",
        "        feature_names.extend([f\"CTDC_{p}_c1\", f\"CTDC_{p}_c2\", f\"CTDC_{p}_c3\"])\n",
        "        feature_names.extend([f\"CTDT_{p}_c1221\", f\"CTDT_{p}_c1331\", f\"CTDT_{p}_c2332\"])\n",
        "        feature_names.extend([f\"CTDD_{p}_cutoff{i}\" for i in range(1, 16)])\n",
        "\n",
        "    # GAAC features\n",
        "    feature_names.extend([f\"GAAC_{g}\" for g in group.keys()])\n",
        "\n",
        "    # GDPC features\n",
        "    groupKey = group.keys()\n",
        "    dipeptide = [f\"{g1}.{g2}\" for g1 in groupKey for g2 in groupKey]\n",
        "    feature_names.extend([f\"GDPC_{d}\" for d in dipeptide])\n",
        "\n",
        "    # GTPC features\n",
        "    triple = [f\"{g1}.{g2}.{g3}\" for g1 in groupKey for g2 in groupKey for g3 in groupKey]\n",
        "    feature_names.extend([f\"GTPC_{t}\" for t in triple])\n",
        "\n",
        "    # PAAC features\n",
        "    dataFile = '/content/PAAC.txt'\n",
        "    with open(dataFile) as f:\n",
        "        records = f.readlines()\n",
        "    AA = ''.join(records[0].rstrip().split()[1:])\n",
        "    feature_names.extend([f\"PAAC_{aa}\" for aa in AA])\n",
        "    for n in range(1, min(3, len(AA))):\n",
        "        feature_names.extend([f\"PAAC_theta_{n}\"])\n",
        "\n",
        "    return feature_names\n",
        "\n",
        "\n",
        "\n",
        "def get_features(fastas):\n",
        "    def AAC():\n",
        "        encoding = []\n",
        "        for sequence in fastas:\n",
        "            count = Counter(sequence)\n",
        "            for key in count:\n",
        "                count[key] = count[key]/len(sequence) * 100\n",
        "            code = []\n",
        "            for aa in AA:\n",
        "                code.append(count[aa])\n",
        "            encoding.append(code)\n",
        "        return encoding\n",
        "\n",
        "    def CTDC(p):\n",
        "        encodings = []\n",
        "        for sequence in fastas:\n",
        "            code = []\n",
        "            c1 = Count_1(group1[p], sequence)/len(sequence)*100\n",
        "            c2 = Count_1(group2[p], sequence)/len(sequence)*100\n",
        "            c3 = 100 - c1 - c2\n",
        "            code = code + [c1, c2, c3]\n",
        "            encodings.append(code)\n",
        "        return encodings\n",
        "\n",
        "\n",
        "    def CTDT(p):\n",
        "        encodings = []\n",
        "        for sequence in fastas:\n",
        "            code = []\n",
        "            aaPair = [sequence[j:j + 2] for j in range(len(sequence) - 1)]\n",
        "            if not aaPair:\n",
        "              code = [0,0,0]\n",
        "            else:\n",
        "              c1221, c1331, c2332 = 0, 0, 0\n",
        "              for pair in aaPair:\n",
        "                  if (pair[0] in group1[p] and pair[1] in group2[p]) or (\n",
        "                          pair[0] in group2[p] and pair[1] in group1[p]):\n",
        "                      c1221 = c1221 + 1\n",
        "                      continue\n",
        "                  if (pair[0] in group1[p] and pair[1] in group3[p]) or (\n",
        "                          pair[0] in group3[p] and pair[1] in group1[p]):\n",
        "                      c1331 = c1331 + 1\n",
        "                      continue\n",
        "                  if (pair[0] in group2[p] and pair[1] in group3[p]) or (\n",
        "                          pair[0] in group3[p] and pair[1] in group2[p]):\n",
        "                      c2332 = c2332 + 1\n",
        "              code = code + [c1221/len(aaPair)*100, c1331/len(aaPair)*100, c2332/len(aaPair)*100]\n",
        "            encodings.append(code)\n",
        "        return encodings\n",
        "\n",
        "    def CTDD(p):\n",
        "        encodings = []\n",
        "        for sequence in fastas:\n",
        "            code = []\n",
        "            code = code + Count_2(group1[p], sequence) + Count_2(group2[p], sequence) + Count_2(group3[p], sequence)\n",
        "            encodings.append(code)\n",
        "        return encodings\n",
        "\n",
        "    def GAAC():\n",
        "        encoding = []\n",
        "        groupKey = group.keys()\n",
        "        for sequence in fastas:\n",
        "            code = []\n",
        "            count = Counter(sequence)\n",
        "            myDict = {}\n",
        "            for key in groupKey:\n",
        "                for aa in group[key]:\n",
        "                    myDict[key] = myDict.get(key, 0) + count[aa]\n",
        "            for key in groupKey:\n",
        "                code.append(myDict[key] / len(sequence))\n",
        "            encoding.append(code)\n",
        "        return encoding\n",
        "\n",
        "    def GDPC():\n",
        "        groupKey = group.keys()\n",
        "        #baseNum = len(groupKey)\n",
        "        dipeptide = [g1 + '.' + g2 for g1 in groupKey for g2 in groupKey]\n",
        "        index = {}\n",
        "        for key in groupKey:\n",
        "            for aa in group[key]:\n",
        "                index[aa] = key\n",
        "        encodings = []\n",
        "        for sequence in fastas:\n",
        "            code = []\n",
        "            myDict = {}\n",
        "            for t in dipeptide:\n",
        "                myDict[t] = 0\n",
        "\n",
        "            sum = 0\n",
        "            for j in range(len(sequence) - 2 + 1):\n",
        "                myDict[index[sequence[j]] + '.' + index[sequence[j + 1]]] = myDict[index[sequence[j]] + '.' + index[\n",
        "                    sequence[j + 1]]] + 1\n",
        "                sum = sum + 1\n",
        "\n",
        "            if sum == 0:\n",
        "                for t in dipeptide:\n",
        "                    code.append(0)\n",
        "            else:\n",
        "                for t in dipeptide:\n",
        "                    code.append(myDict[t] / sum)\n",
        "            encodings.append(code)\n",
        "        return encodings\n",
        "\n",
        "    def GTPC():\n",
        "        groupKey = group.keys()\n",
        "        baseNum = len(groupKey)\n",
        "        triple = [g1 + '.' + g2 + '.' + g3 for g1 in groupKey for g2 in groupKey for g3 in groupKey]\n",
        "        index = {}\n",
        "        for key in groupKey:\n",
        "            for aa in group[key]:\n",
        "                index[aa] = key\n",
        "        encodings = []\n",
        "\n",
        "        for sequence in fastas:\n",
        "            code = []\n",
        "            myDict = {}\n",
        "            for t in triple:\n",
        "                myDict[t] = 0\n",
        "\n",
        "            sum = 0\n",
        "            for j in range(len(sequence) - 3 + 1):\n",
        "                myDict[index[sequence[j]] + '.' + index[sequence[j + 1]] + '.' + index[sequence[j + 2]]] = myDict[index[sequence[j]] + '.' +index[sequence[j + 1]] + '.' +index[sequence[j + 2]]] + 1\n",
        "                sum = sum + 1\n",
        "            if sum == 0:\n",
        "                for t in triple:\n",
        "                    code.append(0)\n",
        "            else:\n",
        "                for t in triple:\n",
        "                    code.append(myDict[t] / sum)\n",
        "            encodings.append(code)\n",
        "        return encodings\n",
        "\n",
        "    def Rvalue(aa1, aa2, AADict, Matrix):\n",
        "        return sum([(Matrix[i][AADict[aa1]] - Matrix[i][AADict[aa2]]) ** 2 for i in range(len(Matrix))]) / len(Matrix)\n",
        "\n",
        "\n",
        "    def PAAC():\n",
        "        dataFile = '/content/PAAC.txt' #r'PAAC.TXT'\n",
        "        with open(dataFile) as f:\n",
        "            records = f.readlines()\n",
        "        AA = ''.join(records[0].rstrip().split()[1:])\n",
        "        # AA = \"ARNDCQEGHILKMFPSTWYV\"\n",
        "        AADict = {}\n",
        "        for i in range(len(AA)):  # 20\n",
        "            AADict[AA[i]] = i\n",
        "        AAProperty = []\n",
        "        AAPropertyNames = []\n",
        "        for i in range(1, len(records)):  # llen(records) 4\n",
        "            array = records[i].rstrip().split() if records[i].rstrip() != '' else None\n",
        "            AAProperty.append([float(j) for j in array[1:]])\n",
        "            AAPropertyNames.append(array[0])\n",
        "\n",
        "        AAProperty1 = []\n",
        "        for i in AAProperty:\n",
        "            meanI = sum(i) / 20\n",
        "            fenmu = math.sqrt(sum([(j - meanI) ** 2 for j in i]) / 20)\n",
        "            AAProperty1.append([(j - meanI) / fenmu for j in i])\n",
        "\n",
        "        encodings = []\n",
        "\n",
        "        for sequence in fastas:\n",
        "            code = []\n",
        "            theta = []\n",
        "            for n in range(1, min(3, len(sequence))):\n",
        "            #for n in range(1, 3):\n",
        "                theta.append(\n",
        "                    sum([Rvalue(sequence[j], sequence[j + n], AADict, AAProperty1) for j in\n",
        "                         range(len(sequence) - n)]) / (\n",
        "                            len(sequence) - n))\n",
        "            myDict = {}\n",
        "            for aa in AA:\n",
        "                myDict[aa] = sequence.count(aa)\n",
        "            code = code + [myDict[aa] / (1 + 0.05 * sum(theta)) for aa in AA]\n",
        "            code = code + [(0.05 * j) / (1 + 0.05 * sum(theta)) for j in theta]\n",
        "            encodings.append(code)\n",
        "        return encodings\n",
        "\n",
        "    print('Feature extraction...')\n",
        "    encoding = []\n",
        "    encoding.append(AAC())\n",
        "    for p in property:\n",
        "        encoding.append(CTDC(p))\n",
        "        encoding.append(CTDT(p))\n",
        "        encoding.append(CTDD(p))\n",
        "    encoding.append(GAAC())\n",
        "    encoding.append(GDPC())\n",
        "    encoding.append(GTPC())\n",
        "    encoding.append(PAAC())\n",
        "    return np.column_stack(encoding)\n",
        "\n",
        "ittca_features_train = get_features(fastas_train)\n",
        "ittca_features_test = get_features(fastas_test)\n",
        "chemical_feature_names = get_feature_names()\n",
        "\n",
        "print(\"There are 365 features for every sequence as you see below:\")\n",
        "print(\"Shape of chemical features in train set:\", ittca_features_train.shape)\n",
        "print(\"Shape of chemical features in test set:\", ittca_features_test.shape)\n",
        "print(\"Feature names:\", chemical_feature_names)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4yWSJ2vhVIr"
      },
      "outputs": [],
      "source": [
        "# 1024 train feature ı 3d array yapma\n",
        "X_train_expanded = []\n",
        "\n",
        "for matrix in X_train:\n",
        "\n",
        "    new_matrix = np.zeros((50, 1024))\n",
        "    new_matrix[:matrix.shape[0], :matrix.shape[1]] = matrix\n",
        "    X_train_expanded.append(new_matrix)\n",
        "\n",
        "\n",
        "\n",
        "X_train_expanded_3d = np.stack(X_train_expanded, axis = 0)\n",
        "print(X_train_expanded_3d.shape)\n",
        "\n",
        "X_train_expanded_11 = []\n",
        "\n",
        "for matrix in X_train_expanded_3d:\n",
        "    new_matrix = matrix[:11, :]   # Her bir matrisin ilk 1024 sütununu seç\n",
        "    X_train_expanded_11.append(new_matrix)\n",
        "\n",
        "X_train_expanded_11 = np.stack(X_train_expanded_11, axis=0)\n",
        "print(X_train_expanded_11.shape)\n",
        "\n",
        "#1024 test feature ı 3d array yapma\n",
        "X_test_expanded = []\n",
        "\n",
        "for matrix in X_test:\n",
        "    new_matrix = np.zeros((50, 1024))\n",
        "    new_matrix[:matrix.shape[0], :matrix.shape[1]] = matrix\n",
        "    X_test_expanded.append(new_matrix)\n",
        "\n",
        "X_test_expanded_3d = np.stack(X_test_expanded, axis = 0)\n",
        "print(X_test_expanded_3d.shape)\n",
        "\n",
        "\n",
        "X_test_expanded_11 = []\n",
        "\n",
        "for matrix in X_test_expanded_3d:\n",
        "    new_matrix = matrix[:11, :]   # Her bir matrisin ilk 1024 sütununu seç\n",
        "    X_test_expanded_11.append(new_matrix)\n",
        "\n",
        "X_test_expanded_11 = np.stack(X_test_expanded_11, axis=0)\n",
        "print(X_test_expanded_11.shape)\n",
        "\n",
        "#print(X_train_expanded_11)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLk1x4bDiUlu"
      },
      "source": [
        "### Data Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXt2_EseiSyW"
      },
      "outputs": [],
      "source": [
        "#Data Scaling\n",
        "#3d data için scaling yapıldı ama kullanmıyoruz onu sadece ittca rf nin scaling ini kullanıcaz\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"Before\")\n",
        "print(\"Min of chemical train features Before Scaling:\", ittca_features_train.min())\n",
        "print(\"Max of chemical train features Before Scaling:\", ittca_features_train.max())\n",
        "print(\"Min of chemical test features Before Scaling:\", ittca_features_test.min())\n",
        "print(\"Max of chemical test features Before Scaling:\", ittca_features_test.max())\n",
        "\n",
        "scaler = StandardScaler()\n",
        "ittca_features_train_scaled = scaler.fit_transform(ittca_features_train)\n",
        "ittca_features_test_scaled = scaler.fit_transform(ittca_features_test)\n",
        "\n",
        "print(\"After\")\n",
        "print(\"Min of chemical train features After Scaling:\", ittca_features_train_scaled.min())\n",
        "print(\"Max of chemical train features After Scaling:\", ittca_features_train_scaled.max())\n",
        "print(\"Min of chemical test features After Scaling:\", ittca_features_test_scaled.min())\n",
        "print(\"Max of chemical test features After Scaling:\", ittca_features_test_scaled.max())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJLlbuaUv3bQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "#kendi featurelarımız + stdev ler 2048 feature , random forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "rf_classifier = RandomForestClassifier(max_depth = 15, min_samples_split = 3, n_estimators = 150)\n",
        "rf_classifier.fit(X_train_final, y_train)\n",
        "y_pred = rf_classifier.predict(X_test_final)\n",
        "\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_rep)\n",
        "\n",
        "#kendi featurelarımız + stdev ler 2048 feature , random forest without hyperparameter tuned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJujXdXWv0wq"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#kendi featurelarımız + stdev ler 2048 features , xgboost\n",
        "from xgboost import XGBClassifier\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "xgb_classifier = XGBClassifier(learning_rate= 0.1, max_depth= 3, n_estimators= 200, random_state=0)\n",
        "xgb_classifier.fit(X_train_final, y_train)\n",
        "y_pred_xgb = xgb_classifier.predict(X_test_final)\n",
        "\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "classification_rep_xgb = classification_report(y_test, y_pred_xgb)\n",
        "\n",
        "print(\"XGBoost Accuracy:\", accuracy_xgb)\n",
        "print(\"XGBoost Classification Report:\\n\", classification_rep_xgb)\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJtxYJQhlyio"
      },
      "source": [
        "## DeepT-i (Inception Module Based 2D CNN with deep features from ProtBERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnq0k6y2jps3"
      },
      "outputs": [],
      "source": [
        "#0.8729 accuracy\n",
        "\n",
        "def create_inception_11(input_layer):\n",
        "    # 1x1 conv\n",
        "    conv1 = Conv2D(32, (1,1), padding='same', activation='relu')(input_layer)\n",
        "    # 3x3 conv\n",
        "    conv3 = Conv2D(16, (3,3), padding='same', activation='relu')(conv1)\n",
        "    # MaxPooling\n",
        "    pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(conv3)\n",
        "    # Concatenate filters\n",
        "    out = Concatenate()([conv1, conv3, pool])\n",
        "    return out\n",
        "\n",
        "def inception_11(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    x = create_inception_11(input_layer)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(2, activation='softmax')(x)  # Assuming binary classification\n",
        "    model = Model(inputs=input_layer, outputs=x)\n",
        "    return model\n",
        "\n",
        "# Define model\n",
        "input_shape = (11, 1024, 1)  # Example input shape\n",
        "model = inception_11(input_shape)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "adam_optimizer = Adam(learning_rate=0.00005)\n",
        "model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, min_delta=0.01, verbose=1)\n",
        "\n",
        "\n",
        "y_train_categorical = to_categorical(y_train, num_classes=2)\n",
        "y_test_categorical = to_categorical(y_test, num_classes=2)\n",
        "\n",
        "history_11 = model.fit(X_train_expanded_11, y_train_categorical, epochs=95, batch_size=948, validation_split=0.1)\n",
        "model.evaluate(X_test_expanded_11, y_test_categorical)\n",
        "\n",
        "y_prob11 = model.predict(X_test_expanded_11)\n",
        "\n",
        "fpr1, tpr1, thresholds = roc_curve(y_test, y_prob11[:, 1])\n",
        "auc1 = roc_auc_score(y_test, y_prob11[:, 1])\n",
        "print('AUC: %0.2f' % auc1)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Predict class labels on the test set\n",
        "y_pred = model.predict(X_test_expanded_11)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels\n",
        "\n",
        "# Ensure y_test is in the correct format, i.e., it should be a single column of labels, not one-hot encoded\n",
        "if y_test_categorical.shape[1] > 1:\n",
        "    y_test_classes = np.argmax(y_test_categorical, axis=1)\n",
        "else:\n",
        "    y_test_classes = y_test  # Assuming y_test is already in the form of class labels\n",
        "\n",
        "# Print the classification report\n",
        "report = classification_report(y_test_classes, y_pred_classes)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN56G6BEmoZC"
      },
      "source": [
        "## DeepT-Hybrid (Inception Module Based 2D CNN with hybrid features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRl337jJmnGR"
      },
      "outputs": [],
      "source": [
        "def create_inception_11_final(input_layer):\n",
        "    # 1x1 conv\n",
        "    conv1 = Conv2D(32, (1,1), padding='same', activation='relu')(input_layer)\n",
        "    # 3x3 conv\n",
        "    conv3 = Conv2D(16, (3,3), padding='same', activation='relu')(conv1)\n",
        "    # MaxPooling\n",
        "    pool = MaxPooling2D((2,2), strides=(1,1), padding='same')(conv3)\n",
        "    # Concatenate filters\n",
        "    out = Concatenate()([conv1, conv3, pool])  # Fixed this\n",
        "    return out\n",
        "\n",
        "def inception_11_final(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    x = create_inception_11_final(input_layer)\n",
        "    cnn_out = Flatten()(x)\n",
        "   # cnn_out = Dropout(0.2)(x)\n",
        "\n",
        "    extra_features_input = Input(shape=(365,))\n",
        "\n",
        "    combined = Concatenate()([cnn_out, extra_features_input])\n",
        "    combined_out = Dense(64, activation='relu')(combined)\n",
        "    final_output = Dense(2, activation='softmax')(combined_out)  # Use softmax for binary classification\n",
        "\n",
        "    my_adam = Adam(learning_rate=0.00005)\n",
        "    model = Model(inputs=[input_layer, extra_features_input], outputs=final_output)\n",
        "    model.compile(optimizer=my_adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define model\n",
        "input_shape = (11, 1024, 1)  # Example input shape\n",
        "model = inception_11_final(input_shape)\n",
        "model.summary()\n",
        "\n",
        "y_train_categorical = to_categorical(y_train, num_classes=2)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, min_delta=0.01, verbose=1)\n",
        "\n",
        "history_11_final = model.fit([X_train_expanded_11, ittca_features_train_scaled], y_train_categorical, epochs=60, batch_size=948, validation_split=0.1)\n",
        "\n",
        "y_test_categorical = to_categorical(y_test, num_classes=2)\n",
        "model.evaluate([X_test_expanded_11, ittca_features_test_scaled], y_test_categorical)\n",
        "\n",
        "y_prob11_final = model.predict([X_test_expanded_11, ittca_features_test_scaled])\n",
        "\n",
        "fpr2, tpr2, thresholds = roc_curve(y_test, y_prob11_final[:, 1])\n",
        "auc2 = roc_auc_score(y_test, y_prob11_final[:, 1])\n",
        "print('AUC: %0.2f' % auc2)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Predict class labels on the test set\n",
        "y_pred_final = model.predict([X_test_expanded_11, ittca_features_test_scaled])\n",
        "y_pred_final_classes = np.argmax(y_pred_final, axis=1)  # Convert probabilities to class labels\n",
        "\n",
        "# Ensure y_test is in the correct format, i.e., it should be a single column of labels, not one-hot encoded\n",
        "if y_test_categorical.shape[1] > 1:\n",
        "    y_test_classes = np.argmax(y_test_categorical, axis=1)\n",
        "else:\n",
        "    y_test_classes = y_test  # Assuming y_test is already in the form of class labels\n",
        "\n",
        "# Print the classification report\n",
        "report_final = classification_report(y_test_classes, y_pred_final_classes)\n",
        "print(report_final)\n",
        "\n",
        "model.save('model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghzaMCa3KWov"
      },
      "outputs": [],
      "source": [
        "\"\"\"import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from keras.models import load_model\n",
        "\n",
        "# Modeli dosyadan yükleyin\n",
        "model = load_model('model.h5')\n",
        "\n",
        "def visualize_model_networkx(model):\n",
        "    G = nx.DiGraph()\n",
        "    for layer in model.layers:\n",
        "        G.add_node(layer.name)\n",
        "        # Katmanın inbound_nodes özelliğini kontrol edin\n",
        "        for node in layer._inbound_nodes:\n",
        "            # inbound_layers tek bir katman veya bir liste olabilir\n",
        "            inbound_layers = node.inbound_layers\n",
        "            if isinstance(inbound_layers, list):\n",
        "                # Eğer liste ise, listedeki her katman için kenar ekleyin\n",
        "                for inbound_layer in inbound_layers:\n",
        "                    if inbound_layer is not None:\n",
        "                        G.add_edge(inbound_layer.name, layer.name)\n",
        "            else:\n",
        "                # Liste değilse, doğrudan bir kenar ekleyin\n",
        "                if inbound_layers is not None:\n",
        "                    G.add_edge(inbound_layers.name, layer.name)\n",
        "\n",
        "    # Grafiği çizdir\n",
        "    pos = nx.spring_layout(G)\n",
        "    nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=2500, edge_color='k', linewidths=1, font_size=10)\n",
        "    plt.show()\n",
        "\n",
        "# Modelin NetworkX ile görsel temsilini çizdirin\n",
        "visualize_model_networkx(model)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tur6OQcv4scA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Concatenate, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import Image\n",
        "\n",
        "\n",
        "def create_inception_11_final(input_layer):\n",
        "    # 1x1 conv\n",
        "    conv1 = Conv2D(32, (1,1), padding='same', activation='relu')(input_layer)\n",
        "    # 3x3 conv\n",
        "    conv3 = Conv2D(16, (3,3), padding='same', activation='relu')(conv1)\n",
        "\n",
        "\n",
        "    # MaxPooling\n",
        "    pool = MaxPooling2D((2,2), strides=(1,1), padding='same')(conv3)\n",
        "    # Concatenate filters\n",
        "    out = Concatenate()([conv1, conv3, pool])\n",
        "    return out\n",
        "\n",
        "def inception_11_final(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    x = create_inception_11_final(input_layer)\n",
        "    cnn_out = Flatten()(x)\n",
        "\n",
        "    extra_features_input = Input(shape=(365,))\n",
        "\n",
        "    combined = Concatenate()([cnn_out, extra_features_input])\n",
        "    combined_out = Dense(64, activation='relu')(combined)\n",
        "    final_output = Dense(2, activation='softmax')(combined_out)\n",
        "\n",
        "    my_adam = Adam(learning_rate=0.00005)\n",
        "    model = Model(inputs=[input_layer, extra_features_input], outputs=final_output)\n",
        "    model.compile(optimizer=my_adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define model\n",
        "input_shape = (11, 1024, 1)\n",
        "model = inception_11_final(input_shape)\n",
        "\n",
        "# Plot the model and save to file\n",
        "plot_model(model, to_file='model_diagram.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# Display the model diagram\n",
        "Image('model_diagram.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzFCocZxrTCQ"
      },
      "source": [
        "##iTTCA-RF (Random forest with chemical features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSPIzZyzrSQt"
      },
      "outputs": [],
      "source": [
        "#ittca features random forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "rf_classifier = RandomForestClassifier(max_depth= 20, min_samples_leaf= 1, min_samples_split= 3, n_estimators= 200, random_state=0)\n",
        "rf_classifier.fit(ittca_features_train, y_train)\n",
        "y_pred = rf_classifier.predict(ittca_features_test)\n",
        "y_prob = rf_classifier.predict_proba(ittca_features_test)[:, 1]\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_rep)\n",
        "\n",
        "\n",
        "\n",
        "fpr3, tpr3, thresholds = roc_curve(y_test, y_prob)\n",
        "auc3 = roc_auc_score(y_test, y_prob)\n",
        "print('AUC: %0.2f' % auc3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4luErTawewV"
      },
      "outputs": [],
      "source": [
        "ittca_features_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLeXjxzlrh7v"
      },
      "source": [
        "#Scaled Chemical Features Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmA6qx4erbYe"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#ittca features random forest with scaled features kullanılmayacak\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "rf_classifier = RandomForestClassifier(random_state=0)\n",
        "rf_classifier.fit(ittca_features_train_scaled, y_train)\n",
        "y_pred = rf_classifier.predict(ittca_features_test_scaled)\n",
        "\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_rep)\n",
        "\n",
        "fpr4, tpr4, thresholds = roc_curve(y_test, y_pred[:, 1])\n",
        "auc4 = roc_auc_score(y_test, y_prob11[:, 1])\n",
        "print('AUC: %0.2f' % auc4)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYPGnZktvhVW"
      },
      "source": [
        "###X_flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNwFw7xDv2xM"
      },
      "outputs": [],
      "source": [
        "\n",
        "lengths_train = [len(seq) for seq in X_train]\n",
        "\n",
        "\n",
        "max_length_train = max(lengths_train)\n",
        "\n",
        "\n",
        "lengths_test = [len(seq) for seq in X_test]\n",
        "\n",
        "\n",
        "max_length_test = max(lengths_test)\n",
        "\n",
        "\n",
        "max_length = max(max_length_train, max_length_test)\n",
        "\n",
        "def pad_and_flatten(sequences, max_length):\n",
        "    padded_sequences = []\n",
        "    for seq in sequences:\n",
        "        # Diziyi padding ile doldur\n",
        "        padding_length = max_length - len(seq)\n",
        "        # Sıfır vektörlerle doldur\n",
        "        padded_seq = np.concatenate([seq, np.zeros((padding_length, seq.shape[1]))], axis=0)\n",
        "        # Düzleştirilmiş hali\n",
        "        flattened_seq = padded_seq.flatten()\n",
        "        padded_sequences.append(flattened_seq)\n",
        "    return np.array(padded_sequences)\n",
        "\n",
        "X_train_flat = pad_and_flatten(X_train, max_length)\n",
        "X_test_flat = pad_and_flatten(X_test, max_length)\n",
        "\n",
        "print(len(X_train))\n",
        "print(X_train_flat.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie0Rf1Q3wK6n"
      },
      "source": [
        "# RF + DeepFeatures Stdevs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To7ZTIWWsNFx"
      },
      "outputs": [],
      "source": [
        "#ProtBERT features + stdevs -> 2048 feature , random forest hyperparameter tuned\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [150],\n",
        "    'max_depth': [10],\n",
        "    'min_samples_split': [3]\n",
        "}\n",
        "\"\"\"\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'max_depth': [10, 15, 20],\n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'min_samples_leaf': [1, 2, 3]\n",
        "}\"\"\"\n",
        "\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_final, y_train)\n",
        "\n",
        "print(\"Best Params: \", grid_search.best_params_)\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "y_pred1 = best_rf_model.predict(X_test_final)\n",
        "y_prob1 = best_rf_model.predict_proba(X_test_final)[:, 1]\n",
        "classification_rep = classification_report(y_test, y_pred1)\n",
        "print(classification_rep)\n",
        "\n",
        "fpr6, tpr6, thresholds = roc_curve(y_test, y_prob1)\n",
        "auc6 = roc_auc_score(y_test, y_prob1)\n",
        "print('AUC: %0.2f' % auc6)\n",
        "#kendi featurelarımız + stdev ler 2048 feature , random forest hyperparameter tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHJd8QVpr0Ih"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#ittca random forest hyperparameter tuned\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'max_depth': [10, 15, 20],\n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'min_samples_leaf': [1, 2, 3]\n",
        "}\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(ittca_features_train_scaled, y_train)\n",
        "\n",
        "print(\"En iyi parametreler: \", grid_search.best_params_)\n",
        "#print(\"En iyi doğruluk skoru: \", grid_search.best_score_)\n",
        "\n",
        "\n",
        "# En iyi modeli alın\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "\n",
        "y_pred = best_rf_model.predict(ittca_features_test_scaled)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(classification_rep)\n",
        "\n",
        "fpr7, tpr7, thresholds = roc_curve(y_test, y_prob11[:, 1])\n",
        "auc7 = roc_auc_score(y_test, y_prob11[:, 1])\n",
        "print('AUC: %0.2f' % auc7)\n",
        "#ittca random forest hyperparameter tuned\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjnkicDqrtK3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#ittca xgboost hyperparameter tuned\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "param_grid_xgb = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'colsample_bytree': [0.5, 0.7, 1.0]\n",
        "}\n",
        "\n",
        "\n",
        "xgb = XGBClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=5, scoring='accuracy')\n",
        "grid_search.fit(ittca_features_train, y_train)\n",
        "\n",
        "print(\"En iyi parametreler: \", grid_search.best_params_)\n",
        "\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_xgb_model.predict(ittca_features_test)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(classification_rep)\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZewY_lJHAR7b"
      },
      "source": [
        "## DeepT-RF (Random Forest with deep features from ProtBERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrjY-rnmv66P"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=150,\n",
        "    max_depth=15,\n",
        "    min_samples_split=3,\n",
        "    random_state=56\n",
        ")\n",
        "\n",
        "\n",
        "rf_classifier.fit(X_train_flat, y_train)\n",
        "\n",
        "y_pred = rf_classifier.predict(X_test_flat)\n",
        "y_prob_rf = rf_classifier.predict_proba(X_test_flat)[:, 1]\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Güncellenmiş Random Forest sınıflandırıcısının doğruluğu:\", accuracy)\n",
        "\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "fpr5, tpr5, thresholds = roc_curve(y_test, y_prob_rf)\n",
        "auc5 = roc_auc_score(y_test, y_prob_rf)\n",
        "print('AUC: %0.2f' % auc5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzqvYcRpsJBx"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#ProtBERT features + stdevs -> 2048 feature , xgboost hyperparameter tuned\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "param_grid_xgb = {\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'n_estimators': [100, 150],\n",
        "    'max_depth': [3, 6]\n",
        "}\n",
        "\n",
        "param_grid_xgb = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'colsample_bytree': [0.5, 0.7, 1.0]\n",
        "}\n",
        "\n",
        "xgb = XGBClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_final, y_train)\n",
        "\n",
        "print(\"En iyi parametreler: \", grid_search.best_params_)\n",
        "\n",
        "\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_xgb_model.predict(X_test_final)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(classification_rep)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2qI55N5Cc62"
      },
      "source": [
        "## DeepT-XGB (XGBoost with deep features from ProtBERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH_0uoAaZStw"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "!pip install shap\n",
        "import shap\n",
        "\n",
        "\n",
        "xgb_classifier = xgb.XGBClassifier(random_state=42, learning_rate = 0.1, n_estimators = 150, max_depth = 9 )\n",
        "\n",
        "xgb_classifier.fit(X_train_flat, y_train)\n",
        "\n",
        "y_pred_xgb = xgb_classifier.predict(X_test_flat)\n",
        "\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(\"XGBoost sınıflandırıcısının doğruluğu:\", accuracy_xgb)\n",
        "y_prob_xgb = xgb_classifier.predict_proba(X_test_flat)[:, 1]\n",
        "\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "fpr8, tpr8, thresholds = roc_curve(y_test, y_prob_xgb)\n",
        "auc8 = roc_auc_score(y_test, y_prob_xgb)\n",
        "print('AUC: %0.2f' % auc8)\n",
        "\n",
        "\n",
        "# SHAP analysis\n",
        "explainer = shap.TreeExplainer(xgb_classifier)\n",
        "shap_values = explainer.shap_values(X_test_flat)\n",
        "\n",
        "# Adjust plot size and generate summary plot\n",
        "plt.figure(figsize=(12, 8))  # Adjust the size as needed\n",
        "shap.summary_plot(shap_values, X_test_flat, max_display=30)  # Show 30 features\n",
        "\n",
        "# Adjust plot size and generate feature importance bar plot\n",
        "plt.figure(figsize=(12, 8))  # Adjust the size as needed\n",
        "shap.summary_plot(shap_values, X_test_flat, plot_type=\"bar\", max_display=30)  # Show 30 features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt9Jj57KZ-6j"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZGXl_5WgTxS"
      },
      "source": [
        "## XGB HYBRID Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vwyok6BbLma"
      },
      "outputs": [],
      "source": [
        "#Hybrid features xgboost\n",
        "\n",
        "x_hybrid_train = np.concatenate((X_train_flat, ittca_features_train_scaled), axis=1)\n",
        "x_hybrid_test = np.concatenate((X_test_flat, ittca_features_test_scaled), axis=1)\n",
        "\n",
        "x_hybrid_train.shape\n",
        "\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "\n",
        "\n",
        "xgb_classifier = xgb.XGBClassifier(random_state=42, colsample_bytree= 0.8, learning_rate= 0.01, max_depth= 6, n_estimators= 200, subsample= 0.9) #, colsample_bytree= 1.0, learning_rate= 0.01, max_depth= 5, n_estimators= 100, subsample= 0.8)\n",
        "\n",
        "xgb_classifier.fit(x_hybrid_train, y_train)\n",
        "\n",
        "y_pred_xgb = xgb_classifier.predict(x_hybrid_test)\n",
        "\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(\"XGBoost sınıflandırıcısının doğruluğu:\", accuracy_xgb)\n",
        "y_prob_xgb = xgb_classifier.predict_proba(x_hybrid_test)[:, 1]\n",
        "\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "fpr8, tpr8, thresholds = roc_curve(y_test, y_prob_xgb)\n",
        "auc8 = roc_auc_score(y_test, y_prob_xgb)\n",
        "print('AUC: %0.2f' % auc8)\n",
        "\n",
        "# SHAP analysis\n",
        "explainer = shap.TreeExplainer(xgb_classifier)\n",
        "shap_values = explainer.shap_values(x_hybrid_test)\n",
        "\n",
        "# Create feature names\n",
        "deep_feature_names = [f'ProtBERT Feature {i}' for i in range(20480)]\n",
        "chemical_feature_names = get_feature_names()\n",
        "hybrid_feature_names = deep_feature_names + chemical_feature_names\n",
        "\n",
        "# Adjust plot size and generate summary plot\n",
        "plt.figure(figsize=(12, 8))  # Adjust the size as needed\n",
        "shap.summary_plot(shap_values, x_hybrid_test, feature_names=hybrid_feature_names)\n",
        "\n",
        "# Adjust plot size and generate feature importance bar plot\n",
        "plt.figure(figsize=(12, 8))  # Adjust the size as needed\n",
        "shap.summary_plot(shap_values, x_hybrid_test, feature_names=hybrid_feature_names, plot_type=\"bar\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWhUCIYGitVl"
      },
      "outputs": [],
      "source": [
        "!pip install graphviz pydotplus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZF-rn9LiyCO"
      },
      "outputs": [],
      "source": [
        "import graphviz\n",
        "import pydotplus\n",
        "from xgboost import plot_tree\n",
        "from IPython.display import Image, display\n",
        "from subprocess import run\n",
        "\n",
        "# Assuming xgb_classifier is already trained\n",
        "\n",
        "# Generate the first decision tree visualization\n",
        "dot_data = xgb.to_graphviz(xgb_classifier, num_trees=0)\n",
        "\n",
        "# Convert the dot data to string and adjust line width and font size\n",
        "dot_string = dot_data.source.replace('fontsize=14', 'fontsize=24').replace('penwidth=1', 'penwidth=5')\n",
        "\n",
        "# Ensure that penwidth is set properly if not already present\n",
        "dot_string = dot_string.replace('edge [', 'edge [penwidth=5,')\n",
        "dot_string = dot_string.replace('node [', 'node [penwidth=5,')\n",
        "\n",
        "# Create a graph from the modified dot string\n",
        "graph = pydotplus.graph_from_dot_data(dot_string)\n",
        "\n",
        "# Save the dot file\n",
        "graph.write('xgb_tree_high_dpi.dot')\n",
        "\n",
        "# Use Graphviz to convert the dot file to a high DPI PNG\n",
        "run(['dot', '-Tpng', 'xgb_tree_high_dpi.dot', '-o', 'xgb_tree_high_dpi.png', '-Gdpi=500'])\n",
        "\n",
        "# Load and display the image\n",
        "img = Image(filename='xgb_tree_high_dpi.png')\n",
        "display(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0jD37p9Mrkp"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import graphviz\n",
        "import pydotplus\n",
        "from subprocess import run\n",
        "\n",
        "# Assuming xgb_classifier is already trained\n",
        "# If not, you need to train it as shown below\n",
        "# iris = datasets.load_iris()\n",
        "# X = iris.data\n",
        "# y = iris.target\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# xgb_classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "# xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Generate the first decision tree visualization\n",
        "dot_data = xgb.to_graphviz(xgb_classifier, num_trees=0)\n",
        "\n",
        "# Convert the dot data to string and adjust line width and font size\n",
        "dot_string = dot_data.source.replace('fontsize=16', 'fontsize=40').replace('penwidth=2', 'penwidth=10')\n",
        "\n",
        "# Ensure that penwidth is set properly if not already present\n",
        "dot_string = dot_string.replace('edge [', 'edge [penwidth=10,')\n",
        "dot_string = dot_string.replace('node [', 'node [penwidth=10,')\n",
        "\n",
        "# Create a graph from the modified dot string\n",
        "graph = pydotplus.graph_from_dot_data(dot_string)\n",
        "\n",
        "# Save the dot file\n",
        "graph.write('xgb_tree_high_dpi.dot')\n",
        "\n",
        "# Use Graphviz to convert the dot file to an SVG with specified dimensions\n",
        "run(['dot', '-Tsvg', 'xgb_tree_high_dpi.dot', '-o', 'xgb_tree_high_dpi.svg', '-Gsize=7.52,1.00!'])\n",
        "\n",
        "print(\"SVG file saved as 'xgb_tree_high_dpi.svg'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMPcvR4BK_iQ"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming xgb_classifier is already trained\n",
        "plt.figure(figsize=(100, 30))  # Large figure size for clarity\n",
        "ax = plt.subplot(1, 1, 1)\n",
        "ax.axis('off')  # Turn off the axis\n",
        "\n",
        "# Plot the tree with specific Graphviz options\n",
        "xgb.plot_tree(xgb_classifier, num_trees=0, rankdir='LR', ax=ax,\n",
        "              show_node_labels=True, fontsize=10,\n",
        "              graph_attrs={'nodesep': '0.1', 'ranksep': '0.1', 'bgcolor': 'transparent'},\n",
        "              edge_attrs={'color': '#000000', 'penwidth': '2'})  # Thicker lines and black color\n",
        "\n",
        "plt.subplots_adjust(left=0, right=1, top=1, bottom=0)  # Adjust the plot margins\n",
        "\n",
        "# Save the figure with high DPI\n",
        "plt.savefig('xgboost_tree_optimized.png', dpi=600, bbox_inches='tight', pad_inches=0)\n",
        "plt.close()  # Close the plot to free up memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr7uPTpToefH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_curve, roc_auc_score\n",
        "\n",
        "# Assuming your X_hybrid arrays and y_train, y_test are already defined\n",
        "x_hybrid_train = np.concatenate((X_train_flat, ittca_features_train_scaled), axis=1)\n",
        "x_hybrid_test = np.concatenate((X_test_flat, ittca_features_test_scaled), axis=1)\n",
        "\n",
        "x_hybrid_train.shape\n",
        "\n",
        "# Define and train the XGBoost classifier\n",
        "xgb_classifier = xgb.XGBClassifier(random_state=42, colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.9)\n",
        "xgb_classifier.fit(x_hybrid_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_xgb = xgb_classifier.predict(x_hybrid_test)\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(\"XGBoost classifier accuracy:\", accuracy_xgb)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "# Compute ROC curve and AUC score\n",
        "y_prob_xgb = xgb_classifier.predict_proba(x_hybrid_test)[:, 1]\n",
        "fpr8, tpr8, thresholds = roc_curve(y_test, y_prob_xgb)\n",
        "auc8 = roc_auc_score(y_test, y_prob_xgb)\n",
        "print('AUC: %0.2f' % auc8)\n",
        "\n",
        "# Initialize the SHAP explainer\n",
        "explainer = shap.TreeExplainer(xgb_classifier)\n",
        "shap_values = explainer.shap_values(x_hybrid_test)\n",
        "\n",
        "# Extract the chemical features (20480 to 20845)\n",
        "chemical_features = x_hybrid_test[:, 20480:20845]\n",
        "chemical_shap_values = shap_values[:, 20480:20845]\n",
        "\n",
        "\n",
        "\n",
        "# Adjust plot size and generate summary plot for chemical features\n",
        "plt.figure(figsize=(12, 8))\n",
        "shap.summary_plot(chemical_shap_values, chemical_features, feature_names=feature_names)\n",
        "\n",
        "# Adjust plot size and generate feature importance bar plot for chemical features\n",
        "plt.figure(figsize=(12, 8))\n",
        "shap.summary_plot(chemical_shap_values, chemical_features, feature_names=feature_names, plot_type=\"bar\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzAIIzD_XZ3s"
      },
      "outputs": [],
      "source": [
        "# Initialize the SHAP explainer\n",
        "explainer = shap.TreeExplainer(xgb_classifier)\n",
        "shap_values = explainer.shap_values(x_hybrid_test)\n",
        "\n",
        "# Extract the chemical features (20480 to 20845)\n",
        "chemical_features = x_hybrid_test[:, 20480:20845]\n",
        "chemical_shap_values = shap_values[:, 20480:20845]\n",
        "\n",
        "# Create a feature names list for SHAP plot with original indices\n",
        "feature_names = [f'Feature {i}' for i in range(20480, 20845)]\n",
        "\n",
        "# Adjust plot size and generate summary plot for chemical features\n",
        "plt.figure(figsize=(12, 8))\n",
        "shap.summary_plot(chemical_shap_values, chemical_features, feature_names=feature_names)\n",
        "\n",
        "# Adjust plot size and generate feature importance bar plot for chemical features\n",
        "plt.figure(figsize=(12, 8))\n",
        "shap.summary_plot(chemical_shap_values, chemical_features, feature_names=feature_names, plot_type=\"bar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cz24Itx87gFv"
      },
      "outputs": [],
      "source": [
        "\"\"\"!pip install shap\n",
        "\n",
        "import shap\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "# Define the best parameters found\n",
        "best_params = {\n",
        "    'n_estimators': 300,\n",
        "    'max_depth': 10,\n",
        "    'min_samples_split': 10,\n",
        "    'min_samples_leaf': 2,\n",
        "    'bootstrap': False\n",
        "}\n",
        "\n",
        "# Train the classifier with the best parameters\n",
        "best_rf_classifier = RandomForestClassifier(**best_params, random_state=56)\n",
        "best_rf_classifier.fit(x_hybrid_train, y_train)\n",
        "\n",
        "# Predict using the test data\n",
        "y_pred = best_rf_classifier.predict(x_hybrid_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Updated Random Forest classifier accuracy:\", accuracy)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate AUC\n",
        "y_prob_rf = best_rf_classifier.predict_proba(x_hybrid_test)[:, 1]\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob_rf)\n",
        "auc = roc_auc_score(y_test, y_prob_rf)\n",
        "print('AUC: %0.2f' % auc)\n",
        "\n",
        "# SHAP analysis\n",
        "explainer = shap.Explainer(best_rf_classifier, x_hybrid_train)\n",
        "shap_values = explainer(x_hybrid_test)\n",
        "\n",
        "# Summary plot\n",
        "shap.summary_plot(shap_values, x_hybrid_test)\n",
        "\n",
        "# Feature importance bar plot\n",
        "shap.summary_plot(shap_values, x_hybrid_test, plot_type=\"bar\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97ut8ubU7J9Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FInvmDjVj0d1"
      },
      "source": [
        "## DeepT-HybridRF (RF HYBRİD FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLBd-p2gjz2t"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "rf_classifier = RandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300, random_state=56)\n",
        "\n",
        "\n",
        "rf_classifier.fit(x_hybrid_train, y_train)\n",
        "\n",
        "y_pred = rf_classifier.predict(x_hybrid_test)\n",
        "y_prob_rf = rf_classifier.predict_proba(x_hybrid_test)[:, 1]\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Güncellenmiş Random Forest sınıflandırıcısının doğruluğu:\", accuracy)\n",
        "\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "fpr5, tpr5, thresholds = roc_curve(y_test, y_prob_rf)\n",
        "auc5 = roc_auc_score(y_test, y_prob_rf)\n",
        "print('AUC: %0.2f' % auc5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMsB-xuSmIi3"
      },
      "source": [
        "## DeepT-HybridRF hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQzE_1gomIqX"
      },
      "outputs": [],
      "source": [
        "# %90.6 accuracy\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve\n",
        "\n",
        "# Zamanlayıcıyı başlat\n",
        "start_time = time.time()\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "best_params = None\n",
        "best_score = 0\n",
        "iteration = 0\n",
        "\n",
        "# Her parametre kombinasyonunu deneyin\n",
        "for params in ParameterGrid(param_grid):\n",
        "    iteration += 1\n",
        "    print(f\"Iteration {iteration}: Testing parameters: {params}\")\n",
        "\n",
        "    rf_classifier = RandomForestClassifier(**params, random_state=56)\n",
        "    rf_classifier.fit(x_hybrid_train, y_train)\n",
        "\n",
        "    y_pred = rf_classifier.predict(x_hybrid_test)\n",
        "    score = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Iteration {iteration}: Accuracy: {score}\")\n",
        "\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_params = params\n",
        "\n",
        "# En iyi parametrelerle eğitim\n",
        "print(\"En iyi hyperparameterlar:\", best_params)\n",
        "best_rf_classifier = RandomForestClassifier(**best_params, random_state=56)\n",
        "best_rf_classifier.fit(x_hybrid_train, y_train)\n",
        "\n",
        "# Tahmin ve sonuçlar\n",
        "y_pred = best_rf_classifier.predict(x_hybrid_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "y_prob_rf = best_rf_classifier.predict_proba(x_hybrid_test)[:, 1]\n",
        "\n",
        "print(\"Güncellenmiş Random Forest sınıflandırıcısının doğruluğu:\", accuracy)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# AUC hesaplama\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob_rf)\n",
        "auc = roc_auc_score(y_test, y_prob_rf)\n",
        "print('AUC: %0.2f' % auc)\n",
        "\n",
        "# Zamanlayıcıyı durdur ve süreyi yazdır\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Model eğitimi ve tuning süresi: {elapsed_time:.2f} saniye\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF-7Oa9cY80Q"
      },
      "source": [
        "## DeepT-HybridXGB HYPERparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZ3U_qZHY70N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import time\n",
        "\n",
        "# Zamanlayıcıyı başlat\n",
        "start_time = time.time()\n",
        "\n",
        "# Verileri birleştirme\n",
        "x_hybrid_train = np.concatenate((X_train_flat, ittca_features_train_scaled), axis=1)\n",
        "x_hybrid_test = np.concatenate((X_test_flat, ittca_features_test_scaled), axis=1)\n",
        "\n",
        "# XGBoost sınıflandırıcısı ve hyperparameter tuning için parametreler\n",
        "\n",
        "\"\"\"param_grid = {\n",
        "    'max_depth': [4, 5, 6, 7],\n",
        "    'learning_rate': [0.005, 0.01, 0.02],\n",
        "    'n_estimators': [160, 180, 200, 220],\n",
        "    'colsample_bytree': [0.6, 0.7, 0.8, 0.85],\n",
        "    'subsample': [0.75, 0.8, 0.85, 0.9]\n",
        "}\"\"\" #en iyi sonuclar\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "\n",
        "best_params = None\n",
        "best_score = 0\n",
        "iteration = 0\n",
        "\n",
        "# Her parametre kombinasyonunu deneyin\n",
        "for params in ParameterGrid(param_grid):\n",
        "    iteration += 1\n",
        "    print(f\"Iteration {iteration}: Testing parameters: {params}\")\n",
        "\n",
        "    xgb_classifier = xgb.XGBClassifier(**params, random_state=42)\n",
        "    xgb_classifier.fit(x_hybrid_train, y_train)\n",
        "\n",
        "    y_pred = xgb_classifier.predict(x_hybrid_test)\n",
        "    score = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Iteration {iteration}: Accuracy: {score}\")\n",
        "\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_params = params\n",
        "\n",
        "# En iyi parametrelerle eğitim\n",
        "print(\"En iyi hyperparameterlar:\", best_params)\n",
        "xgb_classifier_best = xgb.XGBClassifier(**best_params, random_state=42)\n",
        "xgb_classifier_best.fit(x_hybrid_train, y_train)\n",
        "\n",
        "# Tahmin ve sonuçlar\n",
        "y_pred_xgb = xgb_classifier_best.predict(x_hybrid_test)\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "y_prob_xgb = xgb_classifier_best.predict_proba(x_hybrid_test)[:, 1]\n",
        "\n",
        "print(\"XGBoost sınıflandırıcısının doğruluğu:\", accuracy_xgb)\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "# Zamanlayıcıyı durdur ve süreyi yazdır\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Model eğitimi ve tuning süresi: {elapsed_time:.2f} saniye\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTHLpPhuwvRZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6), dpi=600)\n",
        "#plt.plot(fpr1, tpr1, label='DeepT-CNN (AUC = %0.2f)' % auc1, linewidth=2)\n",
        "\n",
        "plt.plot(fpr3, tpr3, label='iTTCA-RF (AUC = %0.2f)' % auc3, linewidth=2)\n",
        "plt.plot(fpr2, tpr2, label='DeepT-Hybrid (AUC = %0.2f)' % auc2, linewidth=2)\n",
        "plt.plot(fpr5, tpr5, label='DeepT-HybridRF (AUC = %0.2f)' % auc5, linewidth=2)\n",
        "plt.plot(fpr8, tpr8, label='DeepT-HybridXGB (AUC = %0.2f)' % auc8, linewidth=2)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=2)  # Rastgele şansın eğrisi\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Positive Rate', fontsize=14, fontweight='bold')\n",
        "plt.title('Receiver Operating Characteristic', fontsize=16, fontweight='bold')\n",
        "plt.legend(loc=\"lower right\", fontsize=12, prop={'weight':'bold'})\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHWaDczx_Z5z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}